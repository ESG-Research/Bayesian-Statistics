---
title: "Metropolis-Hastings RandomWalk"
author: "Zhaojie"
date: "8/20/2024"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
#Candidate-generating distribution aim to approximate the target distribution well
#we can employ an independent Metropolis-Hastings algorithm  to sample the target distribution  (independent M-H in which the candidate-generating distribution q does not depend on the previous iteration of the chain)
```


```{r}
#The posterior distribution is our target distribution and the expression is our g(μ) The first thing we can do in R is write a function to evaluate g(μ)
#Because posterior distributions include likelihoods (the product of many numbers that are potentially small), g(μ) might evaluate to such a small number that to the computer, it effectively zero. This will cause a problem when we evaluate the acceptance ratio α
#To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate:

#          log(g(μ))=n(y¯μ−μ2/2)−log(1+μ2)

#This function will require three arguments, 
#    μ
#   ybar
#    n
```

```{r}
lg = function(mu, n, ybar) {
  mu2 = mu^2
  n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)
}
```

```{r}
#Next, let’s write a function to execute the Random-Walk Metropolis-Hastings sampler with normal proposals:
M_H = function(n, ybar, n_iter, mu_init, cand_sd) {
  ## Random-Walk Metropolis-Hastings algorithm
  
  ## step 1, initialize
  mu_out = numeric(n_iter)
  accpt = 0
  mu_now = mu_init
  lg_now = lg(mu=mu_now, n=n, ybar=ybar)
  
  ## step 2, iterate
  for (i in 1:n_iter) {
    ## step 2a
    mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
    
    ## step 2b
    lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
    lalpha = lg_cand - lg_now # log of acceptance ratio
    alpha = exp(lalpha)
    
    ## step 2c
    u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
    if (u < alpha) { # then accept the candidate
      mu_now = mu_cand
      accpt = accpt + 1 # to keep track of acceptance
      lg_now = lg_cand
    }
    
    ## collect results
    mu_out[i] = mu_now # save this iteration's value of mu
  }
  
  ## return a list of output
  list(mu=mu_out, accpt=accpt/n_iter)
}
```

```{r}
#解答，Loop中的Correction到底有什么用？
#Each draw would be a sample from the candidate-generating distribution. The acceptance step in the algorithm is necessary because it acts as a correction, so that the samples reflect the target distribution more than the candidate-generating distribution.
###总而言之，Correction Step就是让Posterior分布更像Target Distribution而不是更像q Proposal Distribution。
###原理g(θ ∗)/g(θ i)是否大于一。即我们知道目标函数的PDF即可以求某一个点概率，但我们无法对该pdf进行积分来获得normalizing的Posterior Distribution Function，因此我们只能去Simulation，而筛选的价值就是找到那些在目标函数g()中概率密度更高的点，弃掉那些在目标函数中概率密度更低的点。有点像梯度下降法。
```

```{r}
#Now, let’s set up the problem.
#对比Priori mu和 Data mu
#We have a array of data (observation)
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
```

```{r}
install.packages("coda")
library("coda")
```

```{r}
#Finally, we’re ready to run the sampler! Let’s use m=1000 iterations and proposal standard deviation (which controls the proposal step size) 0.9 and initial value at the prior median 0

###Posteriori Sampling
set.seed(36)
Post= M_H(n=n, ybar=ybar,n_iter = 1e3,mu_init = 0,cand_sd = 0.9)
#注意cand_sd控制accept rate，sd越大accpt越小，最佳accpt rate 23%~50%
str(Post)
```

```{r}
traceplot(as.mcmc(Post$mu))  #From "coda" Package
```

```{r}
set.seed(36)
Post= M_H(n=n, ybar=ybar,n_iter = 1e3,mu_init = 30,cand_sd = 0.9)
#假使我们这里把mu_init设置为一个远离真实值的数，比如30，观察迭代移动轨迹
str(Post)
traceplot(as.mcmc(Post$mu)) 
```

```{r}
###Post Analysis
Post$mu_keep = Post$mu[-c(1:100)]
str(Post)
```

```{r}
#对比Posteriori和Priori
plot(density(Post$mu_keep),xlim=c(-1,3)) # Posterior for mu
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(ybar, 0, pch=19) # sample mean 
#Recall 刚刚的Priori和Data对比，Priori建议mu=0，Data建议mu=1附近
#可见，Posterior是对二者的折中。
```

```{r}
```{r}
#解答，Loop中的Correction到底有什么用？
#Each draw would be a sample from the candidate-generating distribution. The acceptance step in the algorithm is necessary because it acts as a correction, so that the samples reflect the target distribution more than the candidate-generating distribution.
###总而言之，Correction Step就是让Posterior分布更像Target Distribution而不是更像q Proposal Distribution。
###原理g(θ ∗)/g(θ i)是否大于一。即我们知道目标函数的PDF即可以求某一个点概率，但我们无法对该pdf进行积分来获得normalizing的Posterior Distribution Function，因此我们只能去Simulation，而筛选的价值就是找到那些在目标函数g()中概率密度更高的点，弃掉那些在目标函数中概率密度更低的点。这意味着M-H算法的MCMC甚至不需要真实的PDF，只要一个和真实PDF proportionable的PDF’即可。我们比较的只是点与点的相对Probability Density。这个Algorithm有点像梯度下降法。
```
```

